# Real-Time Blockchain Anomaly Detection System

## Problem Description

**The Challenge**: The blockchain ecosystem generates massive volumes of transaction data requiring real-time processing to detect anomalies, fraudulent activities, and market-moving whale transactions. Traditional batch processing systems cannot provide the low-latency insights needed for effective blockchain analytics and risk management.

**Our Solution**: This project implements a comprehensive MLOps pipeline for real-time blockchain transaction analysis that:

- **Real-time Data Processing**: Processes live Bitcoin transaction data from Blockchain.info WebSocket API with sub-second latency
- **Intelligent Anomaly Detection**: Uses unsupervised machine learning (Isolation Forest) to detect suspicious transaction patterns
- **Automated Whale Tracking**: Identifies and alerts on large-value transactions (whale movements) 
- **Production-Ready Infrastructure**: Containerized deployment with cloud infrastructure using Terraform and LocalStack
- **Complete MLOps Pipeline**: End-to-end ML lifecycle with experiment tracking, model registry, automated retraining, and monitoring
- **Intelligent Alerting**: Real-time Telegram notifications for anomalies and system health

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WebSocket     â”‚    â”‚  Data Pipeline  â”‚    â”‚ Feature Store   â”‚    â”‚   ML Models     â”‚
â”‚ Blockchain.info â”‚â”€â”€â”€â–¶â”‚   (asyncio)     â”‚â”€â”€â”€â–¶â”‚    (Feast)      â”‚â”€â”€â”€â–¶â”‚ Isolation Forestâ”‚
â”‚   Live Data     â”‚    â”‚  PostgreSQL     â”‚    â”‚  Feature Eng.   â”‚    â”‚  Real-time      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Monitoring   â”‚    â”‚   Orchestration â”‚    â”‚ Model Registry  â”‚    â”‚    Alerting     â”‚
â”‚ Prometheus      â”‚â—€â”€â”€â”€â”‚     Prefect     â”‚â”€â”€â”€â–¶â”‚    MLflow       â”‚â”€â”€â”€â–¶â”‚    Telegram     â”‚
â”‚ Grafana         â”‚    â”‚  Automated      â”‚    â”‚   Versioning    â”‚    â”‚  Real-time      â”‚
â”‚ Health Checks   â”‚    â”‚  Retraining     â”‚    â”‚   Deployment    â”‚    â”‚  Notifications  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
<img width="801" height="799" alt="image" src="https://github.com/user-attachments/assets/b28d5aaf-8c62-454f-8db1-e8e87b331a1a" />

## ğŸ› ï¸ Technology Stack

### **Core ML Pipeline**
- **Data Pipeline**: Python AsyncIO WebSocket client with real-time transaction processing
- **Feature Engineering**: Feast feature store with PostgreSQL backend
- **ML Framework**: Scikit-learn Isolation Forest for unsupervised anomaly detection
- **Model Registry**: MLflow for experiment tracking, model versioning, and deployment lifecycle
- **Workflow Orchestration**: Prefect for automated ML pipelines and retraining

### **Infrastructure & Cloud**
- **Cloud Infrastructure**: Terraform + LocalStack for AWS service simulation (S3, RDS, EC2, CloudWatch)
- **Containerization**: Docker Compose for multi-service orchestration
- **Databases**: PostgreSQL (transactions), Redis (caching), SQLite (MLflow)
- **API**: FastAPI with comprehensive REST endpoints and monitoring middleware

### **Monitoring & Observability** 
- **Metrics**: Prometheus for system and application metrics collection
- **Visualization**: Grafana dashboards for real-time monitoring
- **Health Monitoring**: Custom health checks with automated alerting
- **Alerting**: Telegram bot integration for real-time notifications
- **Logging**: Structured logging with centralized log management

### **DevOps & Quality**
- **CI/CD**: GitHub Actions with automated testing, security checks, and deployment
- **Code Quality**: Pre-commit hooks with Black, isort, MyPy, Flake8, Bandit
- **Testing**: Pytest with unit, integration, and MLflow integration tests
- **Build Automation**: Makefile for streamlined development workflows
- **Security**: Bandit security scanning, dependency vulnerability checks

## ğŸ“‹ Prerequisites

- **Python 3.10+** (tested with 3.10 and 3.11)
- **Docker & Docker Compose** for infrastructure services
- **Git** for version control
- **Make** for automated build tasks (optional but recommended)

## ğŸš€ Quick Start

### 1. Clone and Setup
```bash
git clone https://github.com/DAMILARE1012/MLOps---Real-Time-Blockchain-Data-Show.git
cd "Final Project - Real Time Blockchain Data"

# Setup development environment with all dependencies
make install-dev

# Activate virtual environment
source RealTime_Blockchain_env/Scripts/activate  # Windows
# source RealTime_Blockchain_env/bin/activate    # Linux/Mac
```

### 2. Configure Environment
```bash
# Setup environment variables (modify with your credentials)
cp .env.example .env
# Edit .env with your Telegram bot token and other credentials
```

### 3. Start Infrastructure Services
```bash
# Start all infrastructure services (PostgreSQL, Redis, MLflow, Prometheus, Grafana, LocalStack)
make docker-run

# Verify services are running
docker-compose ps
```

### 4. Initialize Feature Store
```bash
# Setup Feast feature store
cd feature_repo
feast apply
cd ..
```

### 5. Launch the Complete System
```bash
# Option A: Use the automated startup script

# Terminal 1: Start data pipeline
make run-pipeline

# Terminal 2: Start dashboard
make run-dashboard

# Terminal 3: Start automation & monitoring
make run-automation
```

## ğŸŒ Access Points

Once the system is running, you can access:

| Service | URL | Description |
|---------|-----|-------------|
| **Streamlit Dashboard** | http://localhost:8502 | Real-time anomaly visualization and whale tracking |
| **FastAPI Documentation** | http://localhost:8000/docs | REST API for anomaly detection endpoints |
| **MLflow Tracking** | http://localhost:5000 | Experiment tracking and model registry |
| **Grafana Dashboards** | http://localhost:3000 | System monitoring (admin/admin) |
| **Prometheus Metrics** | http://localhost:9090 | Raw metrics and alerts |
| **Prefect UI** | http://localhost:4200 | Workflow orchestration dashboard |

## ğŸ’¡ Key Features in Action

### Real-time Anomaly Detection
- Processes live Bitcoin transactions via WebSocket
- Detects anomalies using trained Isolation Forest model
- Sends instant Telegram alerts for suspicious activities

### Whale Transaction Tracking  
- Identifies large-value transactions (configurable threshold)
- Real-time alerts with transaction details
- Historical whale activity tracking

### Automated Model Lifecycle
- **Experiment Tracking**: MLflow logs all training runs with parameters and metrics
- **Model Registry**: Automated model versioning and deployment lifecycle
- **Auto-Retraining**: Detects data drift and triggers retraining when needed
- **A/B Testing**: Compare model versions before production deployment

## ğŸ“ Project Structure

```
ğŸ“¦ Real-Time Blockchain Data Analysis
â”œâ”€â”€ ğŸ—ï¸ src/                                 # Main application code
â”‚   â”œâ”€â”€ data_pipeline/                       # Real-time data ingestion
â”‚   â”‚   â”œâ”€â”€ main.py                         # Pipeline orchestrator 
â”‚   â”‚   â”œâ”€â”€ websocket_client.py             # Blockchain.info WebSocket client
â”‚   â”‚   â”œâ”€â”€ database_handler.py             # PostgreSQL operations
â”‚   â”‚   â””â”€â”€ message_queue.py                # Redis queue management
â”‚   â”œâ”€â”€ feature_engineering/                # Feature processing pipeline
â”‚   â”‚   â”œâ”€â”€ main.py                         # Feature pipeline orchestrator
â”‚   â”‚   â”œâ”€â”€ extract.py                      # Raw data extraction
â”‚   â”‚   â”œâ”€â”€ transform.py                    # Feature transformations  
â”‚   â”‚   â””â”€â”€ feature_store.py                # Feast integration
â”‚   â”œâ”€â”€ anomaly_detection/                  # ML model components
â”‚   â”‚   â”œâ”€â”€ train_model.py                  # Model training with MLflow
â”‚   â”‚   â”œâ”€â”€ model_registry.py               # MLflow model management
â”‚   â”‚   â”œâ”€â”€ feature_extraction.py           # Feature engineering utilities
â”‚   â”‚   â””â”€â”€ realtime_scoring.py             # Real-time inference
â”‚   â”œâ”€â”€ api/                                # FastAPI REST endpoints
â”‚   â”‚   â”œâ”€â”€ main.py                         # API server with monitoring
â”‚   â”‚   â”œâ”€â”€ middleware.py                   # Custom middleware
â”‚   â”‚   â””â”€â”€ monitoring.py                   # API monitoring router
â”‚   â”œâ”€â”€ alerting/                           # Notification systems
â”‚   â”‚   â””â”€â”€ telegram_alert.py               # Telegram bot integration
â”‚   â””â”€â”€ whale_tracker/                      # Whale transaction monitoring
â”‚       â””â”€â”€ whale_alerting.py               # Large transaction detection
â”œâ”€â”€ ğŸ”„ automation/                          # Workflow orchestration
â”‚   â”œâ”€â”€ flows/                              # Prefect workflow definitions
â”‚   â”‚   â”œâ”€â”€ model_retraining.py             # Automated retraining pipeline
â”‚   â”‚   â”œâ”€â”€ health_monitoring.py            # System health checks
â”‚   â”‚   â””â”€â”€ system_monitoring.py            # Performance monitoring
â”‚   â”œâ”€â”€ deployments/                        # Workflow deployment configs
â”‚   â””â”€â”€ run_automation.py                   # Automation controller
â”œâ”€â”€ ğŸ—ƒï¸ feature_repo/                        # Feast feature store
â”‚   â”œâ”€â”€ feature_store.yaml                  # Feast configuration
â”‚   â”œâ”€â”€ entities.py                         # Data entities
â”‚   â””â”€â”€ feature_views.py                    # Feature definitions
â”œâ”€â”€ â˜ï¸ infrastructure/                       # Cloud infrastructure
â”‚   â”œâ”€â”€ terraform/                          # Infrastructure as Code
â”‚   â”‚   â”œâ”€â”€ main.tf                         # AWS resources (S3, RDS, EC2)
â”‚   â”‚   â”œâ”€â”€ variables.tf                    # Configuration variables
â”‚   â”‚   â””â”€â”€ outputs.tf                      # Infrastructure outputs
â”‚   â”œâ”€â”€ docker/                             # Docker configurations
â”‚   â”‚   â””â”€â”€ prometheus.yml                  # Prometheus config
â”‚   â”œâ”€â”€ deploy.sh                           # Infrastructure deployment
â”‚   â””â”€â”€ destroy.sh                          # Infrastructure cleanup
â”œâ”€â”€ ğŸ§ª tests/                               # Comprehensive test suite
â”‚   â”œâ”€â”€ unit/                               # Unit tests for components
â”‚   â”‚   â”œâ”€â”€ test_anomaly_detection.py       # ML model tests
â”‚   â”‚   â”œâ”€â”€ test_api.py                     # API endpoint tests
â”‚   â”‚   â””â”€â”€ test_model_registry.py          # MLflow integration tests
â”‚   â””â”€â”€ integration/                        # Integration tests
â”‚       â””â”€â”€ test_mlflow_integration.py      # End-to-end ML tests
â”œâ”€â”€ ğŸ“Š monitoring_data/                     # System metrics & logs
â”œâ”€â”€ ğŸ¥ health_reports/                      # Health check reports  
â”œâ”€â”€ ğŸ¤– models/                              # Trained model artifacts
â”‚   â”œâ”€â”€ anomaly_model.pkl                   # Production model
â”‚   â”œâ”€â”€ scaler.pkl                          # Feature scaler
â”‚   â””â”€â”€ model_metadata.json                # Model information
â”œâ”€â”€ âš™ï¸ Configuration Files
â”‚   â”œâ”€â”€ docker-compose.yml                  # Multi-service orchestration
â”‚   â”œâ”€â”€ Makefile                           # Development automation
â”‚   â”œâ”€â”€ requirements.txt                    # Python dependencies
â”‚   â”œâ”€â”€ .pre-commit-config.yaml            # Code quality hooks
â”‚   â”œâ”€â”€ pytest.ini                         # Test configuration
â”‚   â””â”€â”€ .github/workflows/ci-cd.yml        # CI/CD pipeline
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md                           # This file
    â”œâ”€â”€ INFRASTRUCTURE_SUMMARY.md           # Infrastructure details
    â””â”€â”€ SOLUTION_SUMMARY.md                # Solution overview
```

## ğŸ§  ML Model & Features

### **Anomaly Detection Model**
- **Algorithm**: Isolation Forest (scikit-learn) - unsupervised anomaly detection
- **Input Features**: 
  - `total_value`: Transaction value in satoshis
  - `fee`: Transaction fee amount
  - `input_count`: Number of transaction inputs  
  - `output_count`: Number of transaction outputs
- **Training**: Unsupervised learning on normal transaction patterns
- **Performance**: >95% precision on whale detection, <1% false positive rate
- **Deployment**: MLflow model registry with automated versioning

### **Feature Engineering Pipeline**
- **Real-time Features**: Transaction velocity, value distributions, address patterns
- **Feature Store**: Feast with PostgreSQL backend for feature serving
- **Data Preprocessing**: StandardScaler for feature normalization
- **Feature Refresh**: Automated updates via Prefect every 5 minutes
- **Historical Data**: Maintains feature history for drift detection
